# Redis的开发规范和常见的问题

### 1. Run-to-Completion in a solo thread

单线程运行问题。

所有用户的event请求来到Redis后交由后端单线程执行。等每个event处理完成后，才处理下一个

<img src="C:\Users\MacroYuan\AppData\Roaming\Typora\typora-user-images\image-20210509141833897.png" alt="image-20210509141833897" style="zoom:200%;" />

### 2. 扩展为集群版，问题可解？

既然一个Redis进程不行，采用分布式方案扩展为集群版可以吗？集群版确实可以解决一部分问题，常见的请求可以分散到不同DB上。

但是，集群版还是解决不了单个DB被卡住的问题，因为Redis的key hash规则是按照外面的一层PK来做的，没有按照里面的子key或者是feild的来做。如果用户调用了跨分片的命令，如`mget`访问到出问题的DB，仍然会block住，问题还是存在。

### 3. Protocol问题 - 大量客户端与引擎Fully-Meshed问题

### 4. Could not get a reasource from the pool







## 阿里Redis规范

推荐：

- 确定场景，是缓存（Cache）还是存储型；
- Cache的使用原则是：“无它也可，有它更强”；
- 永远不要依赖Cache，它会丢，也会被淘汰；
- 优先设计合理的数据结构和逻辑；
- 设计避免bigKey，就避免了80%的问题；
- Keyspace能分开，就多申请几个Redis实例；
- pushsub不适合做消息分发；
- 尽量避免用lua做事务。

不建议

- 我的服务对RT很敏感。>> 低RT能让我的服务运行得更好；
- 我都把存储公用在一个Redis里。 >> 区分Cache和内存数据库用法，区分应用。
- 我有一个打排行榜/大集合/大链表/消息队列；我觉得服务能力足够了。 >> 尽量拆散，服务能力不够可通过分布式集群版可以打散；
- 我有一个特别大的Value，存在redis里，访问能号些。 >> redis吞吐量有瓶颈。

### BigKey

Redis中80%的问题都是由BigKey导致的。

![image-20210509144138092](C:\Users\MacroYuan\AppData\Roaming\Typora\typora-user-images\image-20210509144138092.png)

在上图中，有可能由一个分片内存满了，访问出了问题，但是其他分片却用得很闲。问题分片的访问比较热，造成网卡打满，或者CPU打满，导致限流，服务可能就宕住了。

### Redis LUA JIT





# Redis抢购系统

Redis单机版的IO模型比较简单，通常是由一个IO线程实现所有命令的解析和处理。

问题是如果由一条慢查询命令，其他的查询都要排队。即当一个客户端执行一个命令执行很慢的时候，后面的命令都会被Block。使用Sentinel判活，会导致ping命令也被延迟，ping命令同样受到慢查询的影响，如果引擎被卡住，则ping失败，导致无法判断服务此时是不是可用，因为这是一种误判。

如果此时发现服务器没有响应，我们从Master切换到Slave，结果又发现慢查询拖慢了Slave，这样的话，ping又会误判，导致很难监听服务是不是可靠。

### Make it a cluster

用多个分片组成一个cluster的时候，也是同样的问题。如果其中的某一个分片被慢查询拖慢，比如用户调用了跨分片的命令，如mget，访问到出问题的分片，仍会卡住，会导致后续的所有命令被阻塞。

![image-20210509145810080](C:\Users\MacroYuan\AppData\Roaming\Typora\typora-user-images\image-20210509145810080.png)

### Could not get a reasource from the pool

常见的Redis客户端如Jedis，会配置连接池。业务线程去访问redis时，每一个查询会去池子里取一个长连接进行访问。如果该查询比较慢，连接没有返回，那么会等待很久，因为请求在返回之前这个连接不能被其他线程使用。

如果查询比较慢，会使得每一个业务线程都拿一个新的长连接，这样等待话，会逐渐耗光所有的长连接，导致最终抛出异常——连接池里面没有新的资源。因为Redis服务端是一个单线程，当客户端的一个长连接被一个慢查询阻塞时，后续连接上的请求也无法被及时处理，因为当前连接无法释放给连接池。

之所以使用连接池，是因为Redis协议不支持连接收敛，Message没有ID，所以Request和Response关联不起来。如果实现异步的话，可用每个请求发送的时候，把回调放入一个队列里面（每个连接一个队列），在请求返回之后从队列取出来回调执行，即FIFO模型。但是服务端连接无法让服务端乱序返回，因为乱序在客户端没有办法对应起来。一般客户端的实现，即BIO比较简单，拿一个连接阻塞住，等其返回之后，再给其它线程使用。

但实际上异步也不能提升效率，因为服务端实际上还是只有一个线程，即便客户端对访问方式进行修改，使得多个连接去发请求，但再服务端一样要排队，因为是单线程，所以慢查询依然会阻塞别的长连接。

另外一个很严重的问题是，Redis的线程模型，当IO线程到万以上的时候，性能比较差，如果有2万到3万长连接，性能将会慢到业务难以承受的程度。而业务机器，比如有300~500台，每一台配50个长连接，很容易达到瓶颈。

***补充：再当下Redis协议上实现异步接口的方法***

1. 一个连接分配一个回调队列，再异步请求发出去前，将处理回调放入队列中，等到响应回来后取出回调执行。这个方法比较常见，主流的支持异步请求的客户端一般都这么实现。

2. 有一些取巧的做法，比如使用Multi-Exec以及ping命令包装请求，比如要调用```set k v```这个命令包装为以下形式：

   ```redis
   multi
   ping {id}
   set k v
   exec
   ```

   服务端返回为：

   ```redis
   {id}
   OK
   ```

   这是利用Muti-Exec的原子执行以及ping的参数原样返回的特性再协议中“夹带”消息的ID的方式，比较取巧，也没见客户端这么实现过。



## 秒杀活动

### 秒杀活动特点

- 秒杀前：用户不断刷新商品详情页，页面请求达到瞬时峰值；
- 秒杀开始：用户点击秒杀按钮，下单请求达到瞬时峰值；
- 秒杀后：少部分成功下单用户不断刷新订单或者退单，大部分用户继续刷新商品详情页等待机会。

### 抢购/秒杀场景的一般方法

- 抢购/秒杀其实主要解决的就是热点数据高并发读写的问题

  - 尽可能减少用户到达应用服务端的读写请求（客户端拦截一部分）；
  - 应用到达服务端的请求要减少对后端存储系统的访问（服务端LocalCache拦截一部分）；
  - 需要请求存储系统的请求尽可能减少对数据库的访问（使用Redis拦截绝大多数）；
  - 最终的请求到达数据库（可用消息队列再排个队兜底，万一后端存储系统无响应，应用服务端要有兜底方案）。

- 基本原则

  1. 数据少
  2. 路径短
  3. 禁单点

- 扣库存的时机

  1. 下单减库存（避免恶意下单不付款、保证大并发请求时库存数据不能为负数）；
  2. 付款减库存（下单成功付不了款影响体验）；
  3. 预扣库存超时释放（可以Quartz等框架做，还要做好安全和反作弊）。

  一般都选择第三种，第一种很难避免恶意下单不付款，第二种成功的下单了，但是由于其他下单成功的抢先付款并减库存后使得库存为0，无法付款。两个体验都不好，一般是先预扣库存，这个单子超时会把库存释放掉。结合电视框架做，同时会做好安全与反作弊机制。

- Redis的一般实现方案

  1. String结构

     1. 直接使用incr/decr/incrby/decrby，注意Redis目前不支持上下界的限制；
     2. 如果要避免负数或者关联关系的库存sku扣减只能使用Lua。

  2. List结构

     1. 每个商品是一个List，每个Node是一个库存单位；

     2. 扣库存使用lpop/rpop命令，直到返回nil（key not exist）。

        List缺点比较明显，如：张勇的内存边打，还有如果一次扣减多个，lpop就要调用很多次，对性能很不好。

  3. Set/Hash结构

     1. 一般用来查重，限制用户只能购买指定个数（hincrby计数，hget判断已购买数量）；
     2. 注意要把用户UID映射到多个key来读写，一定不能都放到某一个key里（热点）；因为典型的热带你key的读写瓶颈会直接造成业务瓶颈。

  4. 业务场景允许的情况下，热点商品可以使用多个key：key_1, key_2, key_3 ...

     1. 随机选择；
     2. 用户UID做映射（不同的用户等级也可以设置不同的库存量）。

